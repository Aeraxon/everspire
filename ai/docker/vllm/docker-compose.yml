services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-gemma3-12b
    restart: unless-stopped
    runtime: nvidia  # Use runtime for LXC container compatibility
    network_mode: host
    volumes:
      - vllm-cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - NVIDIA_VISIBLE_DEVICES=1 
    command:
      - --model
      - RedHatAI/gemma-3-12b-it-FP8-dynamic
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --max-model-len
      - "16384"
      - --gpu-memory-utilization
      - "0.45"
    shm_size: '2gb'
    ipc: host

volumes:
  vllm-cache:
    driver: local
