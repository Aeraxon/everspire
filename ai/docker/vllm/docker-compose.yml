services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-gemma3-12b
    restart: unless-stopped
    network_mode: host
    volumes:
      - vllm-cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
    command:
      - --model
      - RedHatAI/gemma-3-12b-it-FP8-dynamic
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --max-model-len
      - "16384"
      - --gpu-memory-utilization
      - "0.45"
#      - --kv-cache-dtype
#      - fp8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    shm_size: '2gb'
    ipc: host

volumes:
  vllm-cache:
    driver: local
